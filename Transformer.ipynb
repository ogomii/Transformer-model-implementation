{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kHslfephvHq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class self_attention(nn.Module):\n",
        "  '''\n",
        "  embeded_dim     = 256 # - number of dimensions of the encoder space\n",
        "  heads_parallel  = 8   # - number of heads\n",
        "  head_dim        = 32  # - dimension of the single head\n",
        "  '''\n",
        "  def __init__(self, embedded_dim, heads_parallel):\n",
        "    super(self).__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.heads_parallel = heads_parallel\n",
        "    self.head_dim = embedded_dim // heads_parallel\n",
        "\n",
        "    #K, Q, V - push through linear layers\n",
        "    self.values   = nn.Linear(self.embedded_dim, self.embedded_dim, bias=False)\n",
        "    self.queries  = nn.Linear(self.embedded_dim, self.embedded_dim, bias=False)\n",
        "    self.keys     = nn.Linear(self.embedded_dim, self.embedded_dim, bias=False)\n",
        "\n",
        "    self.multi_head_out = nn.Linear(self.heads_parallel*self.head_dim, self.embedded_dim)\n",
        "\n",
        "  def forward(self, values, keys, queries, mask):\n",
        "    N = queries.shape[0] # number of training examples\n",
        "    value_len, key_len, queries_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "\n",
        "    values  = values.reshape(N, value_len, self.heads_parallel, self.head_dim)\n",
        "    keys    = keys.reshape(N, value_len, self.heads_parallel, self.head_dim)\n",
        "    queries = queries.reshape(N, value_len, self.heads_parallel, self.head_dim)\n",
        "\n",
        "    #raw_weights shape: (N, heads_parallel, key_len, queries_len)\n",
        "    raw_weights = torch.einsum('nqhd, nkhd->nhqk', [queries, keys])\n",
        "\n",
        "    # if we are going to use maskes attention\n",
        "    if mask is not None:\n",
        "      raw_weights = raw_weights.masked_fill(mask == 0, float(\"-1e10\"))\n",
        "\n",
        "    # work out the weights for attention\n",
        "    attention = torch.softmax(raw_weights / (self.embedded_dim ** (1/2)), dim=3)\n",
        "\n",
        "    # attention dim: (N, heads_parallel, queries_len, key_len)\n",
        "    out = torch.einsum('nhqs, nshd->nqhd', [attention, values])\n",
        "    out.reshape(N, queries_len, self.heads_parallel*self.head_dim)\n",
        "    out = self.multi_head_out(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "L_AqcHQ1h9ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_module(nn.Module):\n",
        "  def __init__(self, embedded_dim, heads_parallel, dropout, forward_dim):\n",
        "    super(self).__init__()\n",
        "    self.en_module_attention = self_attention(embedded_dim, heads_parallel)\n",
        "    self.add_norm_attention = nn.LayerNorm(embedded_dim)\n",
        "    self.add_norm_feed_forward = nn.LayerNorm(embedded_dim)\n",
        "    self.feed_forward_nn = nn.Sequential(\n",
        "        nn.Linear(embedded_dim, forward_dim*embedded_dim),\n",
        "        nn.Relu(),\n",
        "        nn.Linear(forward_dim*embedded_dim, embedded_dim)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, value, key, query, mask): # Q, K, V are vanilla\n",
        "    embedding = value # can be any V, K, Q\n",
        "    attention = self.en_module_attention(value, key, query, mask) # Q, K, V changed only internally\n",
        "    x = self.dropout(self.add_norm_attention(attention + embedding))\n",
        "    forward_nn = self.feed_forward_nn(x)\n",
        "    out = self.droput(self.add_norm_feed_forward(forward_nn + x))\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "eJyjCyu73kHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder(nn.Module):\n",
        "  '''\n",
        "  vocabulary - input vocabul size\n",
        "  device\n",
        "  forward_dim - is the input embedding expansion\n",
        "  max_dim - max size of the sequence (sentence)\n",
        "  '''\n",
        "  def __init__(self, vocabulary, device, embedded_dim, layers, heads_parallel,\n",
        "               forward_dim, dropout, max_dim):\n",
        "    super(self).__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(vocabulary, embedded_dim)\n",
        "    self.position_embedding = nn.Embedding(max_dim, embedded_dim)\n",
        "\n",
        "    self.architecture = nn.ModuleList(\n",
        "        [\n",
        "            encoder_module(embedded_dim, heads_parallel, dropout, forward_dim)\n",
        "            for _ in range(layers)\n",
        "        ]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_dim = x.shape\n",
        "    positions = torch.arange(0, seq_dim).expand(N, seq_dim).to(self.device)\n",
        "    vkq = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.architecture:\n",
        "      vkq = layer(vkq, vkq, vkq)\n",
        "    return vkq\n"
      ],
      "metadata": {
        "id": "VCmNLhki8jz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder_module(nn.Module):\n",
        "  def __init__(self, embedded_dim, heads_parallel, forward_dim, dropout, device):\n",
        "    super(self).__init__()\n",
        "    self.attention = self.attention(embedded_dim, heads_parallel)\n",
        "    self.add_norm_attention = nn.LayerNorm(embedded_dim)\n",
        "    self.add_norm_feed_forward = nn.LayerNorm(embedded_dim)\n",
        "    self.encoder_module = encoder_module(embedded_dim, heads_parallel, dropout, forward_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, value, key, in_mask, target_mask):\n",
        "    embedding = x\n",
        "    attention = self.attention(x, x, x, target_mask)\n",
        "    query = self.dropout(self.add_norm_attention(attention + embedding))\n",
        "    out = self.encoder_module(value, key, query, in_mask)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "pIsUw7ceB5Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder(nn.Module):\n",
        "  def __init__(self,target_vocabulary, embedded_dim, layers, heads_parallel,\n",
        "               forward_dim, dropout, device, max_dim):\n",
        "    super(self).__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(target_vocabulary, embedded_dim)\n",
        "    self.position_embedding = nn.Embedding(max_dim, embedded_dim)\n",
        "\n",
        "    self.architecture = nn.ModuleList(\n",
        "        [\n",
        "            decoder_module(embedded_dim, heads_parallel, forward_dim, dropout, device)\n",
        "            for _ in range(layers)\n",
        "        ]\n",
        "    )\n",
        "    self.forward_nn = nn.Linear(embedded_dim, target_vocabulary)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forwars(self, x, encoder_out, in_mask, target_mask):\n",
        "    N, seq_dim = x.shape\n",
        "    positions = torch.arange(0, seq_dim).expand(N, seq_dim).to(self.device)\n",
        "    vkq = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "    for layer in self.architecture:\n",
        "      out_decoder = layer(vkq, x, encoder_out, encoder_out, in_mask, target_mask)\n",
        "    out_decoder = self.forward_nn(out_decoder)\n",
        "    return out_decoder"
      ],
      "metadata": {
        "id": "ltHIpIfoEPgt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}